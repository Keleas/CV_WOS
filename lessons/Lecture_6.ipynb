{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция №6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном уроке мы познакомимся с тем, как использваоть предопученные нейронные сети с помощью OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видите ли, чтобы получить (правильные) прогнозы из глубоких нейронных сетей, вам сначала нужно предварительно обработать ваши данные.\n",
    "\n",
    "В контексте глубокого обучения и классификации изображений эти задачи предварительной обработки обычно включают в себя:\n",
    "1. Среднее вычитание\n",
    "2. Масштабирование по некоторому фактору\n",
    "\n",
    "Новый модуль OpenCV для глубокой нейронной сети (**dnn**) содержит две функции, которые можно использовать для предварительной обработки изображений и подготовки их к классификации с помощью предварительно обученных моделей глубокого обучения.\n",
    "\n",
    "В сегодняшнем посте мы разберем функции предварительной обработки OpenCV **cv2.dnn.blobFromImage** и **cv2.dnn.blobFromImages** и поймем, как они работают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глубокое изучение: как работает blobFromImage в OpenCV\n",
    "OpenCV предоставляет две функции, облегчающие предварительную обработку изображений для глубокой классификации обучения:\n",
    "* cv2.dnn.blobFromImage\n",
    "* cv2.dnn.blobFromImages\n",
    "\n",
    "Эти две функции выполняют:\n",
    "1. среднее вычитание\n",
    "2. пересчет\n",
    "3. при желании обмен каналов\n",
    "\n",
    "\n",
    "Дальше мы рассмотрим:\n",
    "1. среднее вычитание и масштабирование\n",
    "2. сигнатуру функции каждой функции предварительной обработки глубокого обучения\n",
    "3. изучим эти методы подробно\n",
    "4. применим функции глубокого обучения OpenCV к набору входных изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глубокое обучение и среднее вычитание\n",
    "\n",
    "<img src=\"img/blob_from_images_mean_subtraction.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "На этом рисунке представлено среднее вычитание, где среднее значение RGB (в центре) было вычислено из набора данных изображений и вычтено из исходного изображения (слева), что привело к выходному изображению (справа).\n",
    "\n",
    "Прежде чем мы углубимся в объяснение функций предварительной обработки глубокого обучения OpenCV, нам сначала необходимо понять среднее вычитание. Среднее вычитание используется для борьбы с изменениями освещенности входных изображений в нашем наборе данных. Поэтому мы можем рассматривать среднее вычитание как метод, используемый для помощи нашим сверточным нейронным сетям.\n",
    "\n",
    "Прежде чем мы начнем тренировать нашу глубокую нейронную сеть, мы сначала вычисляем среднюю интенсивность пикселей по всем изображениям в обучающем наборе для каждого из красного, зеленого и синего каналов.\n",
    "\n",
    "Это означает, что в итоге мы получим три переменные:$$\\mu_R,\\ \\mu_G , \\ \\mu_B.$$\n",
    "\n",
    "\n",
    "Обычно результирующие значения представляют собой 3-х кортеж, состоящий из среднего значения красного, зеленого и синего каналов соответственно.\n",
    "\n",
    "Например, средними значениями для обучающего набора ImageNet являются $R = 103.93$, $G = 116.77$ и $B = 123.68$.\n",
    "\n",
    "Когда мы готовы передать изображение через нашу сеть (для обучения или тестирования), мы вычитаем среднее значение $\\mu$ из каждого входного канала входного изображения:\n",
    "$$\n",
    "R = R - \\mu_R\\\\\n",
    "G = G - \\mu_G\\\\\n",
    "B = B - \\mu_B\\\\\n",
    "$$\n",
    "У нас также может быть коэффициент масштабирования $\\sigma$, который добавляет нормализацию:\n",
    "$$\n",
    "R = (R - \\mu_R) / \\sigma\\\\\n",
    "G = (G - \\mu_G) / \\sigma\\\\\n",
    "B = (B - \\mu_B) / \\sigma\\\\\n",
    "$$\n",
    "Значение $\\sigma$ может быть стандартным отклонением по обучающему набору. Тем не менее, $\\sigma$ также может быть установлен вручную (по сравнению с вычисленным), чтобы масштабировать пространство входного изображения в конкретный диапазон - это действительно зависит от архитектуры, от того, как была обучена сеть.\n",
    "\n",
    "Важно отметить, что не все архитектуры с глубоким обучением выполняют среднее вычитание и масштабирование! Перед предварительной обработкой изображений обязательно прочитайте соответствующую публикацию/документацию по используемой вами глубокой нейронной сети.\n",
    "\n",
    "Как вы обнаружите в своем пути глубокого обучения, некоторые архитектуры выполняют только среднее вычитание (тем самым устанавливая $\\sigma = 1$). Другие архитектуры выполняют как среднее вычитание, так и масштабирование. Всегда проверяйте соответствующую публикацию, которую вы используете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции OpenCV blobFromImage и blobFromImages\n",
    "Начнем с обращения к [официальной документации OpenCV](https://docs.opencv.org/trunk/d6/d0f/group__dnn.html#ga33d1b39b53a891e98a654fdeabba22eb) для **cv2.dnn.blobFromImage**:\n",
    "\n",
    "* *[blobFromImage] создает 4-мерное пятно из изображения. Опционально изменяет размеры и обрезает изображение по центру, вычитает средние значения, масштабирует значения по коэффициенту масштабирования, меняет синий и красный каналы.\n",
    "\n",
    "Неформально, BLOB-объект - это просто (потенциально коллекция) изображений с одинаковыми пространственными размерами (то есть, шириной и высотой), одинаковой глубиной (числом каналов), которые должны быть предварительно обработаны одинаковым образом.\n",
    "\n",
    "Функции **cv2.dnn.blobFromImage** и **cv2.dnn.blobFromImages** практически идентичны.\n",
    "\n",
    "Давайте начнем с проверки подписи функции **cv2.dnn.blobFromImage** ниже:\n",
    "\n",
    "**blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)**\n",
    "\n",
    "1. **image** : это входное изображение, которое мы хотим предварительно обработать, прежде чем передать его через нашу глубокую нейронную сеть для классификации.\n",
    "\n",
    "2. **scalefactor** : после выполнения среднего вычитания мы можем по желанию масштабировать изображения. По умолчанию это значение равно 1,0 (то есть без масштабирования), но мы также можем указать другое значение. Также важно отметить, что **scalefactor** должен быть равен $1 / \\sigma$, поскольку мы фактически умножаем входные каналы (после среднего вычитания) на **scalefactor**.\n",
    "\n",
    "3. **size** : здесь мы предоставляем пространственный размер, который ожидает сверточная нейронная сеть. Для большинства современных нейронных сетей это $224\\times224$, $227\\times227$ или $299\\times299$.\n",
    "\n",
    "4. **mean** : это наши средние значения вычитания. Они могут быть 3-мя кортежами RGB-средств или могут быть одним значением, и в этом случае предоставленное значение вычитается из каждого канала изображения. Если вы выполняете среднее вычитание, убедитесь, что вы поставили 3-кортеж в порядке **(R, G, B)**, особенно при использовании поведения **swapRB = True** по умолчанию.\n",
    "\n",
    "5. **swapRB** : OpenCV предполагает, что изображения имеют порядок каналов **BGR**; однако среднее значение предполагает, что мы используем порядок RGB. Чтобы устранить это расхождение, мы можем поменять местами каналы **R** и **B** на изображении, установив для этого значения значение **True**. По умолчанию OpenCV выполняет этот обмен каналов для нас.\n",
    "Функция **cv2.dnn.blobFromImage** возвращает BLOB-объект, который является нашим входным изображением после среднего вычитания, нормализации и переключения каналов.\n",
    "\n",
    "Функция **cv2.dnn.blobFromImages** точно такая же:\n",
    "\n",
    "**blob = cv2.dnn.blobFromImages(images, scalefactor=1.0, size, mean, swapRB=True)**\n",
    "\n",
    "Единственное исключение состоит в том, что мы можем передавать несколько изображений, что позволяет нам пакетно обрабатывать набор изображений.\n",
    "\n",
    "Если вы обрабатываете несколько изображений, обязательно используйте функцию **cv2.dnn.blobFromImages**, так как при этом меньше затрат на вызов функции и вы сможете быстрее обрабатывать изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глубокое обучение с функцией OpenCV blobFromImage\n",
    "Теперь, когда мы изучили функции **blobFromImage** и **blobFromImages**, давайте применим их к нескольким образцам изображений, а затем передадим их через сверточную нейронную сеть для классификации.\n",
    "\n",
    "В качестве предварительного условия вам необходима версия OpenCV 3.3.0 как минимум. NumPy - это зависимость от привязок Python в OpenCV, а imutils $-$ пакет удобных функций, доступных на GitHub и в индексе пакетов Python.\n",
    "\n",
    "Пакет **imutils** можно установить через pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/0c/659c2bdae8e8ca5ef810b9da02db28feaa29ea448ff36b65a1664ff28142/imutils-0.5.2.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Running setup.py bdist_wheel for imutils: started\n",
      "  Running setup.py bdist_wheel for imutils: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\b2\\40\\59\\139d450e68847ef2f27d876d527b13389dac23df0f66526b5d\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkl-random 1.0.1 requires cython, which is not installed.\n",
      "fastai 1.0.18 requires jupyter, which is not installed.\n",
      "fastai 1.0.18 requires torchvision-nightly, which is not installed.\n",
      "tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.15.4 which is incompatible.\n",
      "tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.4.3 which is incompatible.\n",
      "spacy 2.0.16 has requirement regex==2018.01.10, but you'll have regex 2018.8.29 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the class labels from disk\n",
    "PATH_TXT = '..\\\\src\\\\blob-from-images\\\\synset_words.txt'\n",
    "rows = open(PATH_TXT).read().strip().split(\"\\n\")\n",
    "classes = [r[r.find(\" \") + 1:].split(\",\")[0] for r in rows]\n",
    " \n",
    "# load our serialized model from disk\n",
    "PATH_PROTOTXT = '..\\\\src\\\\blob-from-images\\\\bvlc_googlenet.prototxt'\n",
    "PATH_MODEL = '..\\\\src\\\\blob-from-images\\\\bvlc_googlenet.caffemodel'\n",
    "net = cv2.dnn.readNetFromCaffe(PATH_PROTOTXT, PATH_MODEL)\n",
    " \n",
    "# grab the paths to the input images\n",
    "imagePaths = sorted(list(paths.list_images('..\\\\src\\\\blob-from-images\\\\images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\src\\\\blob-from-images\\\\images\\\\beer.png',\n",
       " '..\\\\src\\\\blob-from-images\\\\images\\\\brown_bear.png',\n",
       " '..\\\\src\\\\blob-from-images\\\\images\\\\keyboard.png',\n",
       " '..\\\\src\\\\blob-from-images\\\\images\\\\monitor.png',\n",
       " '..\\\\src\\\\blob-from-images\\\\images\\\\space_shuttle.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы импортируем imutils, numpy и cv2 **(строки 2-4)**.\n",
    "\n",
    "Затем мы читаем synset_words.txt (метки классов ImageNet) и извлекаем классы, наши метки классов, в **строках 7 и 8**.\n",
    "\n",
    "Чтобы загрузить модель нашей модели с диска, мы используем функцию DNN **cv2.dnn.readNetFromCaffe** и указываем bvlc_googlenet.prototxt в качестве параметра имени файла, а bvlc_googlenet.caffemodel в качестве фактического файла модели **(строки 11 и 12)**.\n",
    "\n",
    "*__Примечание.__ Ресурсы для этой части кода вы можете найти в папке src на [гитхабе курса](https://github.com/Keleas/CV_WOS).*\n",
    "\n",
    "Наконец, мы берем пути к входным изображениям в **строке 15**. \n",
    "\n",
    "Далее мы загрузим образы с диска и предварительно обработаем их с помощью **blobFromImage**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Blob: (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# (1) load the first image from disk, (2) pre-process it by resizing\n",
    "# it to 224x224 pixels, and (3) construct a blob that can be passed\n",
    "# through the pre-trained network\n",
    "image = cv2.imread(imagePaths[0])\n",
    "resized = cv2.resize(image, (224, 224))\n",
    "blob = cv2.dnn.blobFromImage(resized, 1, (224, 224), (104, 117, 123))\n",
    "print(\"First Blob: {}\".format(blob.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом блоке мы сначала загружаем image **(строка 20)**, а затем изменяем его размер до $224\\times224$ **(строка 21)**, необходимых размеров входного изображения для **GoogLeNet**.\n",
    "\n",
    "В **строке 22** мы вызываем **cv2.dnn.blobFromImage**, который, как указано в предыдущем разделе, создаст 4-мерный blob -объект для использования в нашей нейронной сети.\n",
    "\n",
    "Давайте напечатаем форму нашего blob-объекта, чтобы позже мы могли проанализировать его **(строка 23)**.\n",
    "\n",
    "Далее, мы будем кормить blob   через GoogLeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d67dc293cd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# the output predicted probabilities for each of the 1,000 ImageNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# set the input to the pre-trained deep learning network and obtain\n",
    "# the output predicted probabilities for each of the 1,000 ImageNet\n",
    "# classes\n",
    "net.setInput(blob)\n",
    "preds = net.forward()\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the probabilities (in descending) order, grab the index of the\n",
    "# top predicted label, and draw it on the input image\n",
    "idx = np.argsort(preds[0])[::-1][0]\n",
    "text = \"Label: {}, {:.2f}%\".format(classes[idx], preds[0][idx] * 100)\n",
    "cv2.putText(image, text, (5, 25),  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы пропускаем BLOB через сеть **(строки 28 и 29)** и получаем прогнозы, preds.\n",
    "\n",
    "Затем мы сортируем preds **(строка 33)** с наиболее достоверными прогнозами в начале списка и генерируем текст метки для отображения на изображении. Текст метки состоит из метки класса и процентного значения прогноза для верхнего прогноза **(строки 34 и 35)**.\n",
    "\n",
    "Оттуда мы пишем text метки в верхней части image **(строки 36 и 37)**, а затем отображаем image на экране и ждем нажатия клавиши, прежде чем двигаться дальше **(строки 40 и 41)**.\n",
    "\n",
    "_Теперь пришло время использовать множественную форму функции **blobFromImage**._\n",
    "\n",
    "Здесь мы сделаем (почти) то же самое, за исключением того, что вместо этого создадим и заполните список images, а затем передадим список в качестве параметра **blobFromImages**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Blob: (4, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# initialize the list of images we'll be passing through the network\n",
    "images = []\n",
    " \n",
    "# loop over the input images (excluding the first one since we\n",
    "# already classified it), pre-process each image, and update the\n",
    "# `images` list\n",
    "for p in imagePaths[1:]:\n",
    "    image = cv2.imread(p)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    images.append(image)\n",
    " \n",
    "# convert the images list into an OpenCV-compatible blob\n",
    "blob = cv2.dnn.blobFromImages(images, 1, (224, 224), (104, 117, 123))\n",
    "print(\"Second Blob: {}\".format(blob.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы инициализируем наш список images **(строка 44)**, а затем, используя imagePaths, читаем, изменяем размер и добавляем image в список **(строки 49-52)**.\n",
    "\n",
    "Используя нарезку списка, мы пропустили первое изображение из imagePaths в **строке 49**.\n",
    "\n",
    "Оттуда мы передаем images в **cv2.dnn.blobFromImages** в качестве первого параметра **в строке 55**. Все остальные параметры для **cv2.dnn.blobFromImages** идентичны **cv2.dnn.blobFromImage** выше.\n",
    "\n",
    "Для последующего анализа мы печатаем **blob.shape** в **строке 56**.\n",
    "\n",
    "Затем мы пропустим blob через GoogLeNet и напишем метку класса и прогноз в верхней части каждого изображения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input to our pre-trained network and obtain the output\n",
    "# class label predictions\n",
    "net.setInput(blob)\n",
    "preds = net.forward()\n",
    " \n",
    "# loop over the input images\n",
    "for (i, p) in enumerate(imagePaths[1:]):\n",
    "    # load the image from disk\n",
    "    image = cv2.imread(p)\n",
    "\n",
    "    # find the top class label from the `preds` list and draw it on\n",
    "    # the image\n",
    "    idx = np.argsort(preds[i])[::-1][0]\n",
    "    text = \"Label: {}, {:.2f}%\".format(classes[idx],\n",
    "        preds[i][idx] * 100)\n",
    "    cv2.putText(image, text, (5, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # display the output image\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обнаружение объектов в режиме реального времени с глубоким обучением и OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_min = 0.2\n",
    "PATH_prototxt = '..\\\\src\\\\real-time-object-detection\\\\MobileNetSSD_deploy.prototxt.txt'\n",
    "PATH_model = '..\\\\src\\\\real-time-object-detection\\\\MobileNetSSD_deploy.caffemodel'\n",
    "PATH_video = None\n",
    "\n",
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(PATH_prototxt, PATH_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PATH_video != None:\n",
    "    vs = cv2.VideoCapture(PATH_video)\n",
    "else:\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = FPS().start()\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream and resize it\n",
    "    # to have a maximum width of 400 pixels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "\n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "        0.007843, (300, 300), 127.5)\n",
    "\n",
    "    # pass the blob through the network and obtain the detections and\n",
    "    # predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > confidence_min:\n",
    "            # extract the index of the class label from the\n",
    "            # `detections`, then compute the (x, y)-coordinates of\n",
    "            # the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "                confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
